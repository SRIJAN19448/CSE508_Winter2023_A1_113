{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d13350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\srija\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import pickle\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6792ba",
   "metadata": {},
   "source": [
    "# Bigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3f31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that generates all bigrams for a given set of tokens and accordingly updates the posting list and the doc frequency corresponding to the bigram.\n",
    "def helper(inv_idx_freq, inv_idx_postings, list_of_tokens, docID):\n",
    "    n = len(list_of_tokens)\n",
    "\n",
    "    for i in range(n-1):\n",
    "        token = (list_of_tokens[i], list_of_tokens[i+1])\n",
    "        posting_list =  inv_idx_postings[token]\n",
    "        if docID in posting_list:\n",
    "            continue\n",
    "        else:\n",
    "            inv_idx_postings[token].append(docID)\n",
    "            inv_idx_freq[token] = inv_idx_freq[token] + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be663840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9343593f1aed416889e191782205399c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_idx_freq  = defaultdict(int) # initializing the doc Frequency dictionary as a default dict which return 0 whenver a key does not exists.\n",
    "inv_idx_postings  = defaultdict(list) # intializing the postings dictionary to return an empty list whenever it is invoked for a key that does not exits.\n",
    "sample = 2\n",
    "docIDs = []\n",
    "folder_path = 'Q1_2_output'\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            contents = file.read()\n",
    "            docID = int(filename[-4:])\n",
    "            docIDs.append(docID)\n",
    "            # print(docID)\n",
    "            # set_of_tokens = set(contents.strip('][').split(','))\n",
    "            list_of_tokens = ast.literal_eval(contents)\n",
    "            helper(inv_idx_freq, inv_idx_postings, list_of_tokens, docID)\n",
    "            # print(f'{filename}: {contents}')\n",
    "            # print(set_of_tokens)\n",
    "            # sample -= 1\n",
    "            # if (sample == 0):\n",
    "            #     break\n",
    "\n",
    "\n",
    "#sort the index in alphabetical order wrt terms\n",
    "inv_idx_postings = dict(sorted(inv_idx_postings.items()))\n",
    "inv_idx_freq = dict(sorted(inv_idx_freq.items()))\n",
    "\n",
    "for key in inv_idx_postings:\n",
    "    inv_idx_postings[key].sort() #sort the indivdual posting lists correponging to each term in the index\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66a89fa6",
   "metadata": {},
   "source": [
    "# Saving Bigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ccf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(inv_idx_postings, open('bi_inv_idx_postings.pkl', 'wb'))\n",
    "pickle.dump(inv_idx_freq, open('bi_inv_idx_freq.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6eb1420",
   "metadata": {},
   "source": [
    "# Positional index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f3c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_helper(list_of_tokens, docID):\n",
    "    global term_doc_pos\n",
    "    global term_doc_freq\n",
    "\n",
    "    n = len( list_of_tokens )\n",
    "\n",
    "    for i in range(n):\n",
    "        token = list_of_tokens[i]\n",
    "        \n",
    "        if token in term_doc_pos:\n",
    "            tdict = term_doc_pos[token]\n",
    "            \n",
    "            if docID in tdict:\n",
    "                tdict[docID].append(i)\n",
    "\n",
    "            else:\n",
    "                tdict[docID] = [i]\n",
    "\n",
    "        else:\n",
    "            term_doc_pos[token] = { docID : [i] }\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5d4ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88526166e8d42739a766e4f41fd76ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for positional index we create 2 dictionaries.\n",
    "\n",
    "# the first dictionary has its keys as the token and the values is also a dictionary where the keys are docIDs and the values is a list of positions of the token in that doc id\n",
    "\n",
    "# the second dictionary has its keys as the token adn the values are the number of docs containing the term.\n",
    "\n",
    "term_doc_pos = {}\n",
    "term_doc_freq = {}\n",
    "\n",
    "sample = 2\n",
    "docIDs = []\n",
    "folder_path = 'Q1_2_output'\n",
    "for filename in tqdm(os.listdir(folder_path)):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            contents = file.read()\n",
    "            docID = int(filename[-4:])\n",
    "            docIDs.append(docID)\n",
    "            list_of_tokens = ast.literal_eval(contents)\n",
    "            pos_helper(list_of_tokens, docID)\n",
    "            \n",
    "\n",
    "for keys in term_doc_pos:\n",
    "    term_doc_freq[keys] = len( term_doc_pos[keys] )\n",
    "\n",
    "#sort the index in alphabetical order wrt terms          \n",
    "term_doc_pos = dict(sorted(term_doc_pos.items()))\n",
    "term_doc_freq = dict(sorted(term_doc_freq.items()))\n",
    "\n",
    "# sorting the internal dictionary\n",
    "for key in term_doc_pos:\n",
    "    term_doc_pos[key] = dict(sorted(term_doc_pos[key].items()))\n",
    "\n",
    "    for key2 in term_doc_pos[key]:\n",
    "        term_doc_pos[key][key2].sort()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6faf53ab",
   "metadata": {},
   "source": [
    "# Saving Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb9006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(term_doc_pos, open('pos_inv_idx_postings.pkl', 'wb'))\n",
    "pickle.dump(term_doc_freq, open('pos_inv_idx_freq.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbd81440",
   "metadata": {},
   "source": [
    "# Loading Bigram Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a22f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx_postings = pickle.load(open(\"bi_inv_idx_postings.pkl\", 'rb'))\n",
    "inv_idx_freq = pickle.load(open(\"bi_inv_idx_freq.pkl\", 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f72e9fcb",
   "metadata": {},
   "source": [
    "# Bigram Inverted Index Operation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37d7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t1_and_t2(posting1, posting2):\n",
    "  \n",
    "  # if any of the postings is empty then their intersection will also be an empty list\n",
    "  if len(posting1) == 0 or len(posting2) == 0:\n",
    "    return [], 0\n",
    "\n",
    "  ptr1 = 0 #pointer that iterates over posting 1\n",
    "  ptr2 = 0 #pointer that iterates over posting 2\n",
    "\n",
    "  comparisons = 0 #intialize number of comparison as 0\n",
    "  ans = []\n",
    "\n",
    "\n",
    "  while ptr1 < len(posting1) and ptr2 < len(posting2):\n",
    "    comparisons += 1 \n",
    "    if posting1[ptr1] ==  posting2[ptr2]: # this means that both the docs contains the word.\n",
    "      ans.append(posting1[ptr1])\n",
    "      ptr1 = ptr1 + 1\n",
    "      ptr2 = ptr2 + 1\n",
    "\n",
    "    elif posting1[ptr1] <  posting2[ptr2]:\n",
    "      ptr1 = ptr1 + 1\n",
    "    \n",
    "    else:\n",
    "      ptr2 = ptr2 + 1\n",
    "\n",
    "  return ans, comparisons\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef7ca117",
   "metadata": {},
   "source": [
    "# Loading Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434aefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_pos = pickle.load(open(\"pos_inv_idx_postings.pkl\", 'rb'))\n",
    "term_doc_freq = pickle.load(open(\"pos_inv_idx_freq.pkl\", 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4af26cce",
   "metadata": {},
   "source": [
    "# Positional Index Operation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40276ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function tell wether the given doc contains the given token at the given position\n",
    "def helper2(docID, token, pos):\n",
    "    global term_doc_pos\n",
    "    if token in term_doc_pos:\n",
    "        tdict =term_doc_pos[token]\n",
    "        if docID in tdict:\n",
    "            positions = tdict[docID]\n",
    "            if pos in positions:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# this function tells wether the given docs is valid or not with respect to the position of tokens \n",
    "def validDocID(docID, tokens, staring_position):\n",
    "    n = len(tokens)\n",
    "    ans = True\n",
    "    for i in range(n):\n",
    "        check = helper2(docID, tokens[i], staring_position+i)\n",
    "        ans =  ans and check\n",
    "\n",
    "    return ans\n",
    "\n",
    "#  perform the search of given token using positional index and return the retrieved docIDs\n",
    "def pos_search(tokens):\n",
    "    global term_doc_pos\n",
    "    ans = []\n",
    "    \n",
    "    if tokens[0] not in term_doc_pos:\n",
    "        return []\n",
    "\n",
    "    # get all the docIDs and the corresponding position indexes, corresponding to this token\n",
    "\n",
    "    tdict1 = term_doc_pos[tokens[0]]\n",
    "\n",
    "    for doc_id in tdict1:\n",
    "        # check if this docID contains all the required token or not at the desired index.\n",
    "        positions = tdict1[doc_id]\n",
    "        for j in positions:\n",
    "            check = validDocID(doc_id, tokens, j)\n",
    "            if (check):\n",
    "                ans.append(doc_id)\n",
    "                break\n",
    "    return ans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c217fc81",
   "metadata": {},
   "source": [
    "# Query Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35fee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(token):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return token.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19c2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    content1 = query.lower()\n",
    "    content1 = contractions.fix(content1)\n",
    "    tokens = word_tokenize(content1)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens1 = [token for token in tokens if token not in stop_words]\n",
    "    tokens2 = list(map(remove_punc, tokens1))\n",
    "    tokens3 = [token for token in tokens2 if token.strip()]\n",
    "    return tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a00792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    bigram_tokens=[]\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        bigram_tokens.append((tokens[i],tokens[i+1]))\n",
    "    return bigram_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8bae1eb",
   "metadata": {},
   "source": [
    "# Query Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "930beabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents retrieved for query 1 using bigram inverted index: 10\n",
      "Names of documents retrieved for query 1 using bigram inverted index: cranfield0025.txt.txt, cranfield0101.txt.txt, cranfield0173.txt.txt, cranfield0193.txt.txt, cranfield0209.txt.txt, cranfield0310.txt.txt, cranfield1231.txt.txt, cranfield1246.txt.txt, cranfield1248.txt.txt, cranfield1262.txt.txt\n",
      "Number of documents retrieved for query 1 using positional index: 9\n",
      "Names of documents retrieved for query 1 using positional index: cranfield0025.txt, cranfield0101.txt, cranfield0173.txt, cranfield0193.txt, cranfield0310.txt, cranfield1231.txt, cranfield1246.txt, cranfield1248.txt, cranfield1262.txt\n"
     ]
    }
   ],
   "source": [
    "doc_names=os.listdir(\"Q1_2_output\")\n",
    "queries=int(input())\n",
    "for i in range(1,queries+1):\n",
    "    # bigram searching\n",
    "    query=input()\n",
    "    query=preprocess_query(query)\n",
    "    query2=query.copy()\n",
    "    query=bigram(query)\n",
    "    q_ptr=1\n",
    "    answer=None\n",
    "    if(query[0] in inv_idx_postings.keys()):\n",
    "        answer=inv_idx_postings[query[0]]\n",
    "    else:\n",
    "        answer=[]\n",
    "    comparisions=0\n",
    "    while(q_ptr<len(query)):\n",
    "        T2=query[q_ptr]\n",
    "\n",
    "        T2_postings=None\n",
    "        if(T2 in inv_idx_postings.keys()):\n",
    "            T2_postings=inv_idx_postings[T2]\n",
    "        else:\n",
    "            T2_postings=[]\n",
    "\n",
    "        answer,temp=t1_and_t2(answer, T2_postings)\n",
    "        comparisions+=temp\n",
    "        q_ptr+=1\n",
    "\n",
    "    print(f\"Number of documents retrieved for query {i} using bigram inverted index:\",len(answer))\n",
    "    names=\", \".join([doc_names[id-1]+\".txt\" for id in answer])\n",
    "    print(f\"Names of documents retrieved for query {i} using bigram inverted index:\",names)\n",
    "\n",
    "    # positional searching\n",
    "    answer_pos = pos_search(query2)\n",
    "    print(f\"Number of documents retrieved for query {i} using positional index:\",len(answer_pos))\n",
    "    names=\", \".join([doc_names[id-1] for id in answer_pos])\n",
    "    print(f\"Names of documents retrieved for query {i} using positional index:\",names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fac5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'surface', 'pressure', 'distribution'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1fe1fbd972c9c9e41d52b71eaeb7501ba05f317696360d05aa1bc95d167ae166"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
